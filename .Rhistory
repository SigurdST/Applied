# Our dataset
apple <- readr::read_csv("apple_dataset_cleaned.csv", show_col_types = FALSE)
View(apple)
library(Factoshiny)
library(tidyverse)
library(corrr)
install.packages('corrr')
library(Factoshiny)
library(tidyverse)
library(corrr)
library(ggcorrplot)
install.packages('ggcorrplot')
library(Factoshiny)
library(tidyverse)
library(corrr)
library(ggcorrplot)
library(factoextra)
library(psych)
install.packages('psych')
library(Factoshiny)
library(tidyverse)
library(corrr)
library(ggcorrplot)
library(factoextra)
library(psych)
apple <- read_csv("Dataset/apple_dataset.csv") %>%
dplyr::select(-A_id) %>%
drop_na() %>%
mutate(Acidity = as.numeric(Acidity),
across(where(is.numeric), ~ scale(.) %>% as.vector))
apple.PCA <- PCA(apple, ncp=4, quali.sup=c(8),graph=FALSE)
par(mar = c(2.6, 4.1, 1.1, 2.1))
ggplot2::ggplot(cbind.data.frame(x=1:nrow(apple.PCA$eig),y=apple.PCA$eig[,2])) + ggplot2::aes(x=x, y=y)+ ggplot2::geom_col(fill="blue") + ggplot2::xlab("Dimension") + ggplot2::ylab("Percentage of variance") + ggplot2::ggtitle("Decomposition of the total inertia") + ggplot2::theme_light() + ggplot2::theme(plot.title = ggplot2::element_text(hjust =0.5)) + ggplot2::scale_x_continuous(breaks=1:nrow(apple.PCA$eig))
correlation_matrix <- cor(apple[,0:7])
ggcorrplot(correlation_matrix, type = "lower", lab = TRUE)
# Extract the eigenvalues from the PCA object
eigenvalues <- apple.PCA$eig
# Create a table from the eigenvalues
eigenvalue_table <- as.data.frame(eigenvalues)
knitr::kable(eigenvalue_table)
plot.PCA(apple.PCA, axes = c(1,2), choix='var')
plot.PCA(apple.PCA, axes = c(1,2), invisible=c('quali','ind.sup'), habillage = 8, label = "none", palette = palette(c("indianred2", "green4")), select = "cos2 0.25", cex=0.6, cex.main=0.6, cex.axis=0.6)
plot.PCA(apple.PCA, axes = c(1,3), choix='var')
plot.PCA(apple.PCA, axes = c(1,3), invisible=c('quali','ind.sup'), habillage = 8, label = "none", palette = palette(c("indianred2", "green4")), select = "cos2 0.25", cex=0.6, cex.main=0.6, cex.axis=0.6)
plot.PCA(apple.PCA, axes = c(1,4), choix='var')
plot.PCA(apple.PCA, axes = c(1,4), invisible=c('quali','ind.sup'), habillage = 8, label = "none", palette = palette(c("indianred2", "green4")), select = "cos2 0.25", cex=0.6, cex.main=0.6, cex.axis=0.6)
apple_pca_alt <- psych::principal(apple[0:7], rotate = "none", nfactors=4)
cor.plot(apple_pca_alt, main = "Variable Correlations with PCs")
## PCA
library(Factoshiny)
library(tidyverse)
library(corrr)
library(ggcorrplot)
library(factoextra)
library(psych)
apple <- read_csv("Dataset/apple_dataset.csv") %>%
dplyr::select(-A_id) %>%
drop_na() %>%
mutate(Acidity = as.numeric(Acidity),
across(where(is.numeric), ~ scale(.) %>% as.vector))
apple.PCA <- PCA(apple, ncp=4, quali.sup=c(8),graph=FALSE)
par(mar = c(2.6, 4.1, 1.1, 2.1))
ggplot2::ggplot(cbind.data.frame(x=1:nrow(apple.PCA$eig),y=apple.PCA$eig[,2])) + ggplot2::aes(x=x, y=y)+ ggplot2::geom_col(fill="blue") + ggplot2::xlab("Dimension") + ggplot2::ylab("Percentage of variance") + ggplot2::ggtitle("Decomposition of the total inertia") + ggplot2::theme_light() + ggplot2::theme(plot.title = ggplot2::element_text(hjust =0.5)) + ggplot2::scale_x_continuous(breaks=1:nrow(apple.PCA$eig))
correlation_matrix <- cor(apple[,0:7])
ggcorrplot(correlation_matrix, type = "lower", lab = TRUE)
# Extract the eigenvalues from the PCA object
eigenvalues <- apple.PCA$eig
# Create a table from the eigenvalues
eigenvalue_table <- as.data.frame(eigenvalues)
knitr::kable(eigenvalue_table)
plot.PCA(apple.PCA, axes = c(1,2), choix='var')
plot.PCA(apple.PCA, axes = c(1,2), invisible=c('quali','ind.sup'), habillage = 8, label = "none", palette = palette(c("indianred2", "green4")), select = "cos2 0.25", cex=0.6, cex.main=0.6, cex.axis=0.6)
plot.PCA(apple.PCA, axes = c(1,3), choix='var')
plot.PCA(apple.PCA, axes = c(1,3), invisible=c('quali','ind.sup'), habillage = 8, label = "none", palette = palette(c("indianred2", "green4")), select = "cos2 0.25", cex=0.6, cex.main=0.6, cex.axis=0.6)
plot.PCA(apple.PCA, axes = c(1,4), choix='var')
plot.PCA(apple.PCA, axes = c(1,4), invisible=c('quali','ind.sup'), habillage = 8, label = "none", palette = palette(c("indianred2", "green4")), select = "cos2 0.25", cex=0.6, cex.main=0.6, cex.axis=0.6)
apple_pca_alt <- psych::principal(apple[0:7], rotate = "none", nfactors=4)
cor.plot(apple_pca_alt, main = "Variable Correlations with PCs")
apple_pca_rotated <- psych::principal(apple[0:7], rotate="varimax", nfactors=4, scores=TRUE)
cor.plot(apple_pca_rotated, main = "Rotated Variable Correlations with PCs")
library(tidyverse)
library(ggplot2)
# load data
apple <- read_csv("Dataset/apple_dataset.csv") %>%
dplyr::select(-A_id) %>%
drop_na() %>%
mutate(Acidity = as.numeric(Acidity),
across(where(is.numeric), ~ scale(.) %>% as.vector))
set.seed(123)
numapple <- apple[,which(names(apple) != "Quality")]
### 1. K-means
# 1.1. within cluster inertia
k_max <- 10 # number of means
j_max <- 50 # number of kmeans
tot_withinss <- rep(NA, j_max)
tot_withinss_mean <- rep(NA, k_max)
for (k in 1:k_max){
for (j in 1:j_max) {
res.kmeans <- kmeans(numapple, k, algorithm="MacQueen")
tot_withinss[j] <- res.kmeans$tot.withinss
}
tot_withinss_mean[k] <- mean(tot_withinss)
}
# plot inertia decay
x <- c(1:k_max)
ggplot(data.frame(x, tot_withinss_mean), aes(x, tot_withinss_mean)) +
geom_line(color = "black", size = 1.5) +
geom_point(color = "red", size = 3) +
labs(x = "Number of clusters", y = "Within Cluster Intertia", title = "K-means Within Cluster Inertia by group number") +
expand_limits(x=1,y=0) +
scale_x_continuous(breaks = seq(min(x), max(x), by = 1))
# 1.2 check difference to true classes
res.kmeans <- kmeans(numapple, 2, algorithm="MacQueen", iter.max = 50)
apple$k_clust <- res.kmeans$cluster
apple$dQuality <- ifelse(apple$Quality == "good", 1, 0)
# table for mean quality of clusters
print(
apple %>%
group_by(k_clust) %>%
summarize(
mean_quality = mean(dQuality),
n = n()
) %>%
round(2)
)
library(FactoMineR)
res.ahc <- HCPC(numapple, nb.clust=-1, kk=1000)
apple$h_clust <- as.integer(res.ahc$data.clust$clust)
print(
apple %>%
group_by(h_clust) %>%
summarize(mean_quality = mean(dQuality), n = n()) %>%
round(2)
)
print(
apple %>%
group_by(h_clust) %>%
summarize(
Mean_quality = mean(dQuality),
Size = mean(Size),
Weight = mean(Weight),
Sweetness = mean(Sweetness),
Crunchiness = mean(Crunchiness),
Juiciness = mean(Juiciness),
Ripeness = mean(Ripeness),
Acidity = mean(Acidity)
) %>%
round(2) %>%
column_to_rownames(var = "h_clust")
)
library(tidyverse)
library(MASS)
library(caret)
library(ROCR)
library(pROC)
apple <- read_csv("Dataset/apple_dataset.csv") %>%
dplyr::select(-A_id) %>%
drop_na() %>%
mutate(Acidity = as.numeric(Acidity),
across(where(is.numeric), ~ scale(.) %>% as.vector))
set.seed(123) # For reproducibility
trainIndex <- createDataPartition(apple$Quality, p = .8, list = FALSE)
df_training <- apple[trainIndex, ]
df_testing <- apple[-trainIndex, ]
fit <- lda(Quality ~ ., data=df_training)
fit # show results
plot(fit)
# Predict using the fitted model
predictions <- predict(fit, df_testing[,-8])
# Evaluate model performance
confusionMatrix <- table(Predicted = predictions$class, Actual = df_testing$Quality)
print(confusionMatrix)
# Calculate accuracy - corrected line
accuracy <- sum(diag(confusionMatrix)) / sum(confusionMatrix)
print(paste("Accuracy:", round(accuracy,2), '%'))
pred_probs <- predict(fit, df_training, type = "response")
pred <- prediction(pred_probs$posterior[,2], df_training$Quality)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorize = TRUE, main = "ROC Curve for Quality Prediction")
perf_auc <- performance(pred, measure = "auc")
auc_value <- perf_auc@y.values[[1]]
auc_value
# Librairies
library(readr)
library(dplyr)
library(rpart)
library(rpart.plot)
library(caret)
library("ROCR")
# Set path to files
# Our dataset
apple <- read_csv("Dataset/apple_dataset.csv") %>%
dplyr::select(-A_id) %>%
drop_na() %>%
mutate(Acidity = as.numeric(Acidity),
across(where(is.numeric), ~ scale(.) %>% as.vector))
# After split datasets
set.seed(123) # For reproducibility
trainIndex <- createDataPartition(apple$Quality, p = .8, list = FALSE)
df_training <- apple[trainIndex, ]
df_testing <- apple[-trainIndex, ]
# Format of our dataframes
library(DT)
styled_dt <- function(df, n=5) {
DT::datatable(df,
extensions = 'Buttons',
rownames = FALSE,
class = 'dataTables_wrapper',
options = list(
scrollX = TRUE,
pageLength = n,
dom = 'Bfrtip',
buttons = c('copy', 'csv', 'excel')
))
}
# Import
apple <- read_csv("Dataset/apple_dataset.csv") %>%
dplyr::select(-A_id) %>%
drop_na() %>%
mutate(Acidity = as.numeric(Acidity),
across(where(is.numeric), ~ scale(.) %>% as.vector))
# Modification of the order of the columns
apple <- apple %>%
dplyr::select(Quality, everything())
# Data table
#styled_dt(apple)
set.seed(123) # For reproducibility
trainIndex <- createDataPartition(apple$Quality, p = .8, list = FALSE)
df_train <- apple[trainIndex, ]
df_test <- apple[-trainIndex, ]
# Specifiying the positive class
df_train$Quality <- factor(df_train$Quality, levels = c("bad", "good"))
# Creating the tree classification
appletree <- rpart(Quality ~ ., data=df_train, method = "class", parms = list(split='gini'))
# Plot of the tree
rpart.plot(appletree, extra = 101)
# Plot of the cp parameters
plotcp(appletree)
# Print cp appletree
printcp(appletree)
# Pruning
appletree_pruned <- prune(appletree, cp = 0.018)
# Plot
rpart.plot(appletree_pruned, extra = 101)
table(predict(appletree_pruned, df_train, type="class"),df_train$Quality)
# Confusion matrix
table(predict(appletree, df_train, type = 'class'), df_train$Quality)
df_train_pred <- predict(appletree, df_train, type = 'class')
df_train_value <- as.factor(df_train$Quality)
confusionMatrix(df_train_pred, df_train_value, positive = "good")
pred_proba <- prediction(predict(appletree,type="prob")[, 2],df_train$Quality)
plot(performance(pred_proba, "tpr", "fpr"),colorize=TRUE)
abline(0, 1, lty = 2)
# Confusion matrix
table(predict(appletree, df_test, type = "class"), df_test$Quality)
df_test_pred <- predict(appletree, df_test, type = 'class')
df_test_value <- as.factor(df_test$Quality)
confusionMatrix(df_test_pred, df_test_value, positive = "good")
# Librairies
library(Factoshiny)
library(tidyverse)
library(corrr)
library(ggcorrplot)
library(factoextra)
library(psych)
# Data import
apple <- read_csv("Dataset/apple_dataset.csv") %>%
dplyr::select(-A_id) %>%
drop_na() %>%
mutate(Acidity = as.numeric(Acidity),
across(where(is.numeric), ~ scale(.) %>% as.vector))
# Split in training and testing data
set.seed(123) # For reproducibility
trainIndex <- createDataPartition(apple$Quality, p = .8, list = FALSE)
df_train <- apple[trainIndex, ]
df_test <- apple[-trainIndex, ]
pred_proba2 <- prediction(predict(appletree, df_test, type="prob")[, 2],df_test$Quality)
plot(performance(pred_proba2, "tpr", "fpr"),colorize=TRUE)
abline(0, 1, lty = 2)
?rpart.plot
?createDataPartition
?rpart
?rpart.plot
```{r, fig.align='center'}
# Librairies
library(Factoshiny)
library(tidyverse)
library(corrr)
library(ggcorrplot)
library(factoextra)
library(psych)
library(caret)
library(rpart)
library(rpart.plot)
# Data import
apple <- read_csv("Dataset/apple_dataset.csv") %>%
dplyr::select(-A_id) %>%
drop_na() %>%
mutate(Acidity = as.numeric(Acidity),
across(where(is.numeric), ~ scale(.) %>% as.vector))
# Split in training and testing data
set.seed(123) # For reproducibility
trainIndex <- createDataPartition(apple$Quality, p = .8, list = FALSE)
df_train <- apple[trainIndex, ]
df_test <- apple[-trainIndex, ]
fit$means # show results
knitr::kable(fit$means# show results)
knitr::kable(fit$means) # show results
fit$means %>% kable('Latex') # show results
fit$means %>% knitr::kable('Latex') # show results
fit$means %>% dplyr::kable('Latex') # show results
fit$means %>% knitr::kable('Latex') # show results
?lda
library(MASS)
?kable_Latex
?rotate_table
??rotate_table
?prediction
