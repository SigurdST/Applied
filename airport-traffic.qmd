---
title: Airport traffic
subtitle: Applied
date: "2023-12-05"
author: 
- name: "Louis Rodriguez"
- name: "Sigurd Saue"
- name: "Adrien Berard"
- name: "Nathan Pizzetta"

format:
  html:
    toc: true
    toc-depth: 3
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(dplyr)
library(zoo)
library(xts)
library(tidyr)
library(magrittr)
library(ggplot2)
library(tseries)
library(forecast)
```


```{r, echo=FALSE,  warning=FALSE, message=FALSE}
# Format of our dataframes
styled_dt <- function(df, n=5) {

  DT::datatable(df, 
  extensions = 'Buttons',
  rownames = FALSE,
  class = 'dataTables_wrapper',
  options = list(
    scrollX = TRUE, 
    pageLength = n,
    dom = 'Bfrtip',
    buttons = c('copy', 'csv', 'excel')
  ))
  }
```

# Data import

:::{.panel-tabset}

### Traffic data
This dataset includes the monthly number of passengers from 1998 to 2023 in different european airports.

```{r, warning=FALSE, message=FALSE}
# Global data
traffic <- openxlsx::read.xlsx(xlsxFile="~/Documents/TSE/Cours/M1/Applied econometrics/Data bases/data_airports_APP.xlsx")

# Alternative
# path_file1 <- file.choose()
# traffic <- openxlsx::read.xlsx(xlsxFile="path_file1")

# Simplified label
traffic <- traffic %>% dplyr::rename("Airport" = "REP_AIRP.(Labels)")
```

### Localisation data
This dataset associates the airport with its country.

```{r}
# Country
airports_names<- read.csv("~/Documents/TSE/Cours/M1/Applied econometrics/Data bases/airports_names.csv")

# Alternative
# path_file2 <- file.choose()
# airports_names<- read.csv("path_fil2")


airports_names$Airport <- paste(airports_names$Airport, "airport", sep = " ")
airports_names <- airports_names %>% mutate(Country = ifelse(Country == "Chile", "Spain", Country))
```

### Members of European Union data
This dataset associates 1 to European Union members and 0 to the rest.

```{r}
# EU
eu_countries_2019 <- read.csv("~/Documents/TSE/Cours/M1/Applied econometrics/eu_countries_2019.csv")

# Alternative
# path_file3 <- file.choose()
# eu_countries <- read.csv("path_file3")
```


:::

&nbsp;

# Pre-processing

We choose to keep the data from 2002 to 2023. Before 2022 we have only few data available and it seems not interesting for our study.

```{r}
# Selection
names_col <- names(traffic)
selected_col <- c(names_col[1], names_col[50:length(names_col)])
traffic <- traffic %>% dplyr::select(all_of(selected_col))
```

&nbsp;

Here we aggregate the country associated to each airport. We will need them for our analysis and to create our segmentation by country afterward.

```{r}
# Merged
traffic_mg <- merge(traffic, airports_names, by = "Airport", all.x = TRUE)

# Saving our new dataset
write.csv(traffic_mg, file = "traffic_airp.csv", row.names = FALSE, na = "NA")
```

&nbsp;

We first check if in our data we have som duplicated lines.
```{r}
airports_dupli <- duplicated(traffic_mg)
length(traffic_mg[airports_dupli,])
```

And then apply `unique`.
```{r}
# Duplicates erase
traffic_mg <- unique(traffic_mg)
```

&nbsp;

We check if some of the airports are not associated with a country in our dataset `airports_names`.

```{r}
# Checking
airports_without_country <- traffic_mg[is.na(traffic_mg$Country), ]
```

```{r, echo=FALSE}
styled_dt(airports_without_country, 5)
```

&nbsp;

Our dataset gives a report of the number of passengers carried by the airports each month starting in January
of 1998 to september of 2023.

```{r, echo=FALSE, warning=FALSE}
styled_dt(traffic_mg, 5)
```

&nbsp;

We there modify our dataset structure to prevent issues with `pivot_longer`.

```{r}
# Modification
traffic_pivot <- tidyr::pivot_longer(traffic_mg, cols = -c("Airport", "Country"), names_to = "Date", values_to = "Passengers")

# Managing Nan
traffic_pivot$Passengers[traffic_pivot$Passengers == ":"] <- 0

# Numerical values
traffic_pivot$Passengers <- as.numeric(traffic_pivot$Passengers)

# Date
traffic_pivot$Date <- zoo::as.Date(paste0(traffic_pivot$Date, "-01"), format="%Y-%m-%d")
```

## Selection of the most relevant european airports

For this, our goal is to keep at least one airport by country. To do so, we will focus on the airports with the most attendace in every country.

&nbsp;

First, we sum the total number of passengers between 2002 and 2023 :
```{r}
# Sum
traffic_sum <- traffic_pivot %>% group_by(Airport, Country) %>% summarise(sumPassengers = sum(Passengers))
```

Then, we select the most relevant airport of every country :
```{r}
# Selection
airports_best_ranked <- traffic_sum %>% group_by(Country) %>% slice_max(order_by = sumPassengers)
```

For 3 countries we have no data. Therefore, we delete them. At the same time, we also erase some territories of no interest and issues. For that we previously checked that in our list we do not have any big airport that could be pertinent.

```{r}
# Erase
airports_best_ranked <- airports_best_ranked %>% filter(sumPassengers != 0)

list_countries = c("Faroe Islands (Denmark)", "Fictional/Private", "French Guiana", 
  "Guadeloupe (France)", "Martinique (France)", "Mayotte (France)", "Reunion (France)",
  "Saint Barthelemy (France)", "Saint Martin (France)", "Svalbard (Norway)", NA)

airports_best_ranked <- airports_best_ranked %>% filter(!(Country %in% list_countries))
```


# Final dataset

Here is our dataset that we will use from now on to build our model and make our analysis.

```{r}
# Final dataset
airports_final_list <- unique(airports_best_ranked$Airport)

traffic_checked <- traffic_pivot %>% filter(Airport %in% airports_final_list)
```


```{r, echo=FALSE}
write.csv(traffic_checked, file = "traffic_airp_final_dataset.csv", row.names = FALSE, na = "NA")
styled_dt(traffic_checked)
```

# Plot of our data

## Overview

Here we visualize how is the general tendance with all our airports.

### EU members

```{r}
# EU
eu_sum <- eu_countries_2019 %>% group_by(EU_2019) %>% count()
eu_sum <- eu_sum %>% mutate(EU_2019 = ifelse(EU_2019 == 1, "Member", "Non member"))

# Plot
ggplot(eu_sum, aes(x = EU_2019, y = n, fill = EU_2019)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = n), vjust = 3, colour = 'white', size = 4) +
  labs(title = "Repartition of european countries", x = "Category", y = "count", fill = "European Union (2019)") +
  theme_minimal()
```


:::{.panel-tabset}

### All airports 1

```{r, echo=FALSE}
ggplot2::ggplot(traffic_checked, aes(x = Date, y = Passengers, color = Country)) +
  geom_line() +
  theme_minimal() +
  labs(title = "Monthly Passengers per Country", x = "Date", y = "Number of Passengers")
```

### All airports 2

We transform our dataset as a time serie
```{r}
traffic_checked_xts <- traffic_checked$Passengers %>% xts::xts(order.by=traffic_checked$Date)
```

```{r, echo=FALSE}
autoplot(traffic_checked_xts) +
  xlab("Date") +
  ylab("Number of passengers") +
  ggtitle("Monthly passengers per country ")
```

### General
```{r}
traffic_mean <- traffic_checked %>%
  group_by(Date) %>%
  summarize(MeanPassengers = mean(Passengers, na.rm = TRUE))

dygraphs::dygraph(traffic_mean, main = "Average Passengers per Month", xlab = "Date")
```

:::

&nbsp;

## Focus

Quick view of the 3 airports with most traffic in our dataset.

:::{.panel-tabset}

### CDG

#### Paris Charles de Gaulles Airport

```{r}
charles <- traffic_checked %>% dplyr::filter(Airport == "PARIS-CHARLES DE GAULLE airport")
```

Formating the dataset as a time serie variable

```{r}
# Time serie function
charles_ts <- charles %>% dplyr::select(4) %>% ts(frequency = 12, start = c(2002, 1))
```

```{r, echo=FALSE}
dygraphs::dygraph(data=charles_ts, main="Passengers per month at Paris Charles de Gaulle")
```

### LHR

#### London Heathrow Airport

```{r}
heathrow <- traffic_checked %>% dplyr::filter(Airport == "LONDON HEATHROW airport")
```

Formating the dataset as a time serie variable

```{r}
# Time serie function
heathrow_ts <- heathrow %>% dplyr::select(4) %>% ts(frequency = 12, start = c(2002, 1))
```

```{r, echo=FALSE}
dygraphs::dygraph(data=heathrow_ts, main="Passengers per month at London Heathrow")
```

### AMS

#### Amsterdam Schiphol airport

```{r}
schiphol <- traffic_checked %>% dplyr::filter(Airport == "AMSTERDAM/SCHIPHOL airport")
```

Formating the dataset as a time serie variable

```{r}
# Time serie function
schiphol_ts <- schiphol %>% dplyr::select(4) %>% ts(frequency = 12, start = c(2002, 1))
```

```{r, echo=FALSE}
dygraphs::dygraph(data=schiphol_ts, main="Passengers per month at Amsterdam-Schiphol")
```

:::

# Our Time Serie model

First of all, we want to keep the data only until the end of 2019, period before COVID impact if we refer to our plots.

```{r}
charles_ts19 <- charles %>% dplyr::select(4) %>% ts(frequency = 12, start = c(2002, 1), end = c(2019, 12))
```


## Study on our data

### Stationarity and parameters

As first though, we saw previously on the graph of the Paris Charles de Gaulle airport that there is an ascending trend. This violates the asumption of same mean at all time in the stationarity properties. Also, distinct seasonal patterns are present which is also a violation of the previous requirement.
We thus already think that our time series are not stationary.

&nbsp;

We will verify it as it follows.

#### ADF
We test wether our time serie is stationary or not with the Augmented Dickey-Fuller Test.

```{r}
charles_ts19 <- na.omit(charles_ts19)
adf_test <- adf.test(charles_ts19, alternative = "stationary")

# Results of the ADF test
print(adf_test)
```
Our result with a $p-value = 0.01 < 0.05$ let us suppose that there is stationarity. We success to reject the null hypothesis that states a unit root (non-stationarity) in the time serie.

&nbsp;

This let us suppose that we do not need to differenciate `charles_ts19`

#### Graphs

On both graphs, the blue dashed lines represent values beyond which the ACF and PACF are significantly different from zero at $5\%$ level. These lines represent the bounds of the $95\%$ confidence intervals.
A bar above are under these lines would suppose a correlation. On the contrary, a bar between these two lines would suppose zero correlation.

:::{.panel-tabset}

#### ACF

ACF for Autocorrelation Function


This graph is useful to identify the number of MA(q) (Moving Average) terms in our model.

We will interpret it as the following :

- If the ACF shows a slow exponential or sinusoidal decay, it could suggests an AR process.
- If the ACF cuts off after a specific delay (lag), it could suggests an AM process.

```{r, echo=FALSE}
# Plot of the Autocorrelation Function (ACF)
forecast::ggAcf(charles_ts19) +
  ggplot2::ggtitle("Sample ACF for CDG airport")
```
The ACF shows a gradual decline but with several significant spikes at various lags.
There are significant lags at intervals that could suggest a seasonal pattern, which is in line with our year seasonality. Our data frequency is monthly, and these intervals correspond to the number 12.

The fact that there are significant autocorrelations at multiple lags might also suggest that our data is not stationary, and differencing (either seasonal or non-seasonal) might be required to achieve stationarity.

#### PACF

PACF for Partial Autocorrelation Function


This graph is useful to identify the number of AR(p) (AutoRegressive) terms in our model.

We will interpret it as the following :

- If the PACF shows a slow exponential or sinusoidal decay, it could indicates an MA process.
- If the PACF cuts off after a certain delay, it could indicates an AR process.

```{r, echo=FALSE}
# Plot of the Partial Autocorrelation Function (PACF)
forecast::ggPacf(charles_ts19) +
  ggplot2::ggtitle("Sample PACF for CDG airport")
```
The PACF shows a significant spike at lag 1 and then cuts off, which indicates an AR(1) component may be present in our time series.
The other lags do not appear to be significantly different from zero, suggesting that no higher-order AR terms are needed.

:::

#### Seasonal differencing

We will take the seasonality into account, which means that we make a difference at lag 12 because of our monthly data.

```{r}
# Difference
charles_ts19_diff <- diff(charles_ts19, lag = 12)
```

After differencing, we check again for stationarity. We thus run the ADF test with the ACF and PACF graphs.

:::{.panel-tabset}

#### ADF

$H_0 : Unit root$
$H_1 : Stationary$

```{r}
adf_test2 <- adf.test(charles_ts19_diff, alternative = "stationary")

# Results of the ADF test
print(adf_test2)
```
We reject the null hypothesis at a $5\%$ significance level.

#### ACF

```{r, echo=FALSE}
# Plot of the Autocorrelation Function (ACF)
forecast::ggAcf(charles_ts19_diff) +
  ggplot2::ggtitle("Sample ACF for CDG airport after differencing")
```



#### PACF

```{r, echo=FALSE}
# Plot of the Partial Autocorrelation Function (PACF)
forecast::ggPacf(charles_ts19_diff) +
  ggplot2::ggtitle("Sample PACF for CDG airport after differencing")
```


:::

Suggested coefficients with an automated function of R : `auto.arima`

```{r}
# Get suggestion
forecast::auto.arima(charles_ts19)
```
### Model

We now construct our model, taking into account our previouse analysis.
We thus have seasonality, stationarity after differencing (taking into account the seasonality), and indications with the ACF and PACf about the value of AR's and MA's parameters.

&nbsp;

Based on ACF and PACF, we decide to start with an AR(1) non-seasonal component, and a seasonal difference.
We also include a seasonal MA(1) component due to the marginal significance at lag 12 in the ACF.


```{r}
# SARIMA
sarima_model <- Arima(charles_ts19, order=c(1,0,0), seasonal=list(order=c(0,1,1), period=12))
```

Now that we have build our model, we check the residuals.

```{r}
checkresiduals(sarima_model)
```
Interpretations :

- **The Ljung-Box test**, we got $p-value = 1.416*10^{-08} < 0.05$ suggest a significant autocorrelation at some of the lags used in the test (up to lag 24 here). Our model may not capture adequately  all the autocorrelation in our data.

- **The residuals time series plot**, helps us to check if our residuals look like white noise (meaning that our model has captured the underlying process correctly). Here, our residuals fluctuate around zero, we do not see obvious patterns which is a good sign.

- **The ACF of residuals**, helps us to check the autocorrelation within our residuals. In a good model we would expect the bars to be between both dashed blue lines. In our case, we see some of the bars outside the confidence intervals. Thus, there may be some autocorrelation in the residuals that our model does not capture.

- **The histogram and density plot**, shows the distribution of the residuals along with the density curve of the normal distribution for comparison. By assumption, our residuals should look like a normal distribution with a mean of zero. Here, we have large pikes around zero, but also some big deviations around. Our residuals may not be normally distributed.


### Prediction

```{r}
# Prediction of the 12 next months
forecasts <- forecast(sarima_model, h=12)
#str(forecasts)
plot(forecasts)
```


