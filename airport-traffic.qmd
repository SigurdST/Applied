---
title: Airport traffic
subtitle: Applied
date: "2023-12-05"
author: 
- name: "Adrien Berard"
- name: "Nathan Pizzetta"
- name: "Louis Rodriguez"
- name: "Sigurd Saue"
format:
  html:
    toc: true
    toc-depth: 3
editor: 
  markdown: 
    wrap: sentence
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(dplyr)
library(zoo)
library(xts)
library(tidyr)
library(magrittr)
library(ggplot2)
library(tseries)
library(forecast)
```

```{r}
# Sourcing the file wich contains the functions to find the best parameters with regard to AIC and BIC
source("hyperparameters-selection.R")
# Loading the best hyperparameters for our model we previously found
load("hyperparameters.RData")
```

```{r, echo=FALSE,  warning=FALSE, message=FALSE}
# Format of our dataframes
styled_dt <- function(df, n=5) {

  DT::datatable(df, 
  extensions = 'Buttons',
  rownames = FALSE,
  class = 'dataTables_wrapper',
  options = list(
    scrollX = TRUE, 
    pageLength = n,
    dom = 'Bfrtip',
    buttons = c('copy', 'csv', 'excel')
  ))
  }
```

# Data import

::: panel-tabset
### Traffic data

This dataset includes the monthly number of passengers from 1998 to 2023 in different european airports.

```{r, warning=FALSE, message=FALSE}
# Global data
traffic <- openxlsx::read.xlsx(xlsxFile="datasets/data_airports_APP.xlsx")

# Simplified label
traffic <- traffic %>% dplyr::rename("Airport" = "REP_AIRP.(Labels)")
```

### Localisation data

This dataset associates the airport with its country.

```{r}
# Country
airports_names<- read.csv("datasets/airports_by_country.csv")

airports_names$Airport <- paste(airports_names$Airport, "airport", sep = " ")
airports_names <- airports_names %>% mutate(Country = ifelse(Country == "Chile", "Spain", Country))
```

### Members of European Union data

This dataset associates 1 to European Union members and 0 to the rest.

```{r}
# EU
eu_countries_2019 <- read.csv("datasets/eu_countries_2019.csv")
```
:::

 

# Pre-processing

We choose to keep the data from 2002 to 2023.
Before 2022 we have only few data available and it seems not interesting for our study.

```{r}
# Selection
names_col <- names(traffic)
selected_col <- c(names_col[1], names_col[50:length(names_col)])
traffic <- traffic %>% dplyr::select(all_of(selected_col))
```

 

Here we aggregate the country associated to each airport.
We will need them for our analysis and to create our segmentation by country afterward.

```{r}
# Merged
traffic_mg <- merge(traffic, airports_names, by = "Airport", all.x = TRUE)
```

 

We first check if in our data we have some duplicated lines.

```{r}
airports_dupli <- duplicated(traffic_mg)
length(traffic_mg[airports_dupli,])
```

And then apply `unique`.

```{r}
# Duplicates erase
traffic_mg <- unique(traffic_mg)
```

 

We check if some of the airports are not associated with a country in our dataset `airports_names`.

```{r}
# Checking
airports_without_country <- traffic_mg[is.na(traffic_mg$Country), ]
```

```{r, echo=FALSE}
styled_dt(airports_without_country, 5)
```

 

Our dataset gives a report of the number of passengers carried by the airports each month starting in January of 1998 to september of 2023.

```{r, echo=FALSE, warning=FALSE}
styled_dt(traffic_mg, 5)
```

 

We there modify our dataset structure to prevent issues with `pivot_longer`.

```{r}
# Modification
traffic_pivot <- tidyr::pivot_longer(traffic_mg, cols = -c("Airport", "Country"), names_to = "Date", values_to = "Passengers")

# Managing Nan
traffic_pivot$Passengers[traffic_pivot$Passengers == ":"] <- 0

# Numerical values
traffic_pivot$Passengers <- as.numeric(traffic_pivot$Passengers)

# Date
traffic_pivot$Date <- zoo::as.Date(paste0(traffic_pivot$Date, "-01"), format="%Y-%m-%d")
```

## Selection of the most relevant european airports

For this, our goal is to keep at least one airport by country.
To do so, we will focus on the airports with the most attendace in every country.

 

First, we sum the total number of passengers between 2002 and 2023 :

```{r}
# Sum
traffic_sum <- traffic_pivot %>% group_by(Airport, Country) %>% summarise(sumPassengers = sum(Passengers))
```

Then, we select the most relevant airport of every country :

```{r}
# Selection
airports_best_ranked <- traffic_sum %>% group_by(Country) %>% slice_max(order_by = sumPassengers)
```

For 3 countries we have no data.
Therefore, we delete them.
At the same time, we also erase some territories of no interest and issues.
For that we previously checked that in our list we do not have any big airport that could be pertinent.

```{r}
# Erase
airports_best_ranked <- airports_best_ranked %>% filter(sumPassengers != 0)

list_countries = c("Faroe Islands (Denmark)", "Fictional/Private", "French Guiana", 
  "Guadeloupe (France)", "Martinique (France)", "Mayotte (France)", "Reunion (France)",
  "Saint Barthelemy (France)", "Saint Martin (France)", "Svalbard (Norway)", NA)

airports_best_ranked <- airports_best_ranked %>% filter(!(Country %in% list_countries))
```

# Final dataset

Here is our dataset that we will use from now on to build our model and make our analysis.

```{r}
# Final dataset
airports_final_list <- unique(airports_best_ranked$Airport)

traffic_checked <- traffic_pivot %>% filter(Airport %in% airports_final_list)
```

```{r, echo=FALSE}
write.csv(traffic_checked, file = "datasets/data_airports_cleaned_APP.csv", row.names = FALSE, na = "NA")
styled_dt(traffic_checked)
```

# Plot of our data

## Overview

Here we visualize how is the general tendance with all our airports.

### EU members

In our data, we do not have only countries taking part of the European Union but also other european countries that are not member of it.
We thus plot there distribution to visualize it.

```{r}
# EU
eu_sum <- eu_countries_2019 %>% group_by(EU_2019) %>% count()
eu_sum <- eu_sum %>% mutate(EU_2019 = ifelse(EU_2019 == 1, "Member", "Non member"))

# Plot
ggplot(eu_sum, aes(x = EU_2019, y = n, fill = EU_2019)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = n), vjust = 3, colour = 'white', size = 4) +
  labs(title = "Repartition of european countries", x = "", y = "Number of countries", fill = "European Union (2019)") +
  theme_minimal()
```

### Ranking on the total airport traffic

We ranked our countries by their total passenger traffic.
This will help us to make an analysis on the most relevant airport of our dataset.

```{r}
ggplot2::ggplot(airports_best_ranked, aes(x = sumPassengers, y = reorder(Country, sumPassengers))) +
  geom_bar(stat = "identity", fill = "green") +
  labs(title = "Ranking of Countries based on their traffic",
       x = "Total passengers carried",
       y = "Country")
```

### Global trend on our data

::: panel-tabset
### Our airports of interest

```{r, echo=FALSE}
# List of the airports
airports_to_keep <- c("PARIS-CHARLES DE GAULLE airport", "ADOLFO SUAREZ MADRID-BARAJAS airport", "ROMA/FIUMICINO airport", "KOBENHAVN/KASTRUP airport", "OSLO/GARDERMOEN airport")

# Filter
filtered_traffic_checked <- traffic_checked %>%
  dplyr::filter(Airport %in% airports_to_keep)

# Plot
ggplot2::ggplot(filtered_traffic_checked, aes(x = Date, y = Passengers, color = Country)) +
  geom_line() +
  theme_minimal() +
  labs(title = "Monthly Passengers per Country", x = "Date", y = "Number of Passengers")
```

### General

```{r}
traffic_mean <- traffic_checked %>%
  group_by(Date) %>%
  summarize(MeanPassengers = mean(Passengers, na.rm = TRUE))

dygraphs::dygraph(traffic_mean, main = "Average Passengers per Month", xlab = "Date")
```
:::


## Focus

Quick view of the 3 airports with most traffic in our dataset.

::: panel-tabset

### PARIS

#### PARIS-CHARLES DE GAULLE airport

```{r}
paris <- traffic_checked %>% dplyr::filter(Airport == "PARIS-CHARLES DE GAULLE airport")
```

Formating the dataset as a time serie variable

```{r}
# Time serie function
paris_ts <- paris %>% dplyr::select(4) %>% ts(frequency = 12, start = c(2002, 1), end = c(2023, 7))
```

```{r, echo=FALSE}
dygraphs::dygraph(data=paris_ts, main="Passengers per month at Paris Charles de Gaulle")
```

### MADRID

#### ADOLFO SUAREZ MADRID-BARAJAS airport

```{r}
madrid <- traffic_checked %>% dplyr::filter(Airport == "ADOLFO SUAREZ MADRID-BARAJAS airport")
```

Formating the dataset as a time serie variable

```{r}
# Time serie function
madrid_ts <- madrid %>% dplyr::select(4) %>% ts(frequency = 12, start = c(2002, 1))
```

```{r, echo=FALSE}
dygraphs::dygraph(data=madrid_ts, main="Passengers per month at Madrid")
```

### ROMA

#### ROMA/FIUMICINO airport

```{r}
roma <- traffic_checked %>% dplyr::filter(Airport == "ROMA/FIUMICINO airport")
```

Formating the dataset as a time serie variable

```{r}
# Time serie function
roma_ts <- roma %>% dplyr::select(4) %>% ts(frequency = 12, start = c(2002, 1))
```

```{r, echo=FALSE}
dygraphs::dygraph(data=roma_ts, main="Passengers per month at Roma")
```

### COPENHAGEN

#### KOBENHAVN/KASTRUP airport

```{r}
copenhagen <- traffic_checked %>% dplyr::filter(Airport == "KOBENHAVN/KASTRUP airport")
```

Formating the dataset as a time serie variable

```{r}
# Time serie function
copenhagen_ts <- copenhagen %>% dplyr::select(4) %>% ts(frequency = 12, start = c(2002, 1))
```

```{r, echo=FALSE}
dygraphs::dygraph(data=copenhagen_ts, main="Passengers per month at Copenhagen")
```

### OSLO

#### OSLO/GARDERMOEN airport

```{r}
oslo <- traffic_checked %>% dplyr::filter(Airport == "OSLO/GARDERMOEN airport")
```

Formating the dataset as a time serie variable

```{r}

# Time serie function
oslo_ts <- oslo %>% dplyr::select(4) %>% ts(frequency = 12, start = c(2002, 1))
```

```{r, echo=FALSE}
dygraphs::dygraph(data=oslo_ts, main="Passengers per month at Oslo Gardermoen")
```
:::

# Our Time Series model

First of all, we want to keep the data only until the end of 2019, period before COVID impact if we refer to our plots.

```{r}
# Paris
paris_ts19 <- paris %>% dplyr::select(4) %>% ts(frequency = 12, start = c(2002, 1), end = c(2019, 12))
paris_ts19 <- na.omit(paris_ts19)
# Madrid
madrid_ts19 <- madrid %>% dplyr::select(4) %>% ts(frequency = 12, start = c(2002, 1), end = c(2019, 12))
madrid_ts19 <- na.omit(madrid_ts19)
# Roma
roma_ts19 <- roma %>% dplyr::select(4) %>% ts(frequency = 12, start = c(2002, 1), end = c(2019, 12))
roma_ts19 <- na.omit(roma_ts19)
# Copenhagen
copenhagen_ts19 <- copenhagen %>% dplyr::select(4) %>% ts(frequency = 12, start = c(2002, 1), end = c(2019, 12))
copenhagen_ts19 <- na.omit(copenhagen_ts19)
# Oslo
oslo_ts19 <- oslo %>% dplyr::select(4) %>% ts(frequency = 12, start = c(2002, 1), end = c(2019, 12))
oslo_ts19 <- na.omit(oslo_ts19)
```

## Study on our data

### Stationarity and parameters

As first though, we saw previously on the graph of the Paris Charles de Gaulle airport that there is an ascending trend.
This violates the assumption of same mean at all time in the stationarity properties.
Also, distinct seasonal patterns are present which is also a violation of the previous requirement.
We thus already think that our time series are not stationary.



The positive trend seems to be linear which suggests that a first difference could be sufficient to detrend it.
In the case of curve we would have preferred to previously transform our data and then make a first difference.
But before doing a first difference we have to take the seasonality into account.
If after the seasonal differencing the trend remains, then we will apply a forst difference.



We will verify it as it follows.



First, we study the non-seasonal behavior.
It is likely that the short run non-seasonal components will contribute to the model.
We thus take a look at the ACF and PACF behavior under the seasonality lag (12) to assess what non-seasonal components might be.

### No differencing

We start with anakysing our time serieswithout previous differencing.

```{r}
# Trend
ts_decomposed <- decompose(paris_ts19)

# Plot
plot(paris_ts19, col = 'blue', xlab = "Year", ylab = "Passengers",
     main = "CDG passengers before seasonal differencing")

lines(ts_decomposed$trend,  col = 'red')

legend("topright", legend = c("Original", "Trend"), col = c("blue", "red"), 
       lty = c(1, 1), cex = 0.8)
```

On both ACF and PACF graphs, the blue dashed lines represent values beyond which the ACF and PACF are significantly different from zero at $5\%$ level.
These lines represent the bounds of the $95\%$ confidence intervals.
A bar above are under these lines would suppose a correlation.
On the contrary, a bar between these two lines would suppose zero correlation.

::: panel-tabset
#### ADF

We test whether our time series is stationary or not with the Augmented Dickey-Fuller Test.

$H_0 : Unit root$ $H_1 : Stationary$

```{r}

adf_test <- adf.test(paris_ts19, alternative = "stationary")

# Results of the ADF test
print(adf_test)
```

Our result with a $p-value = 0.01 < 0.05$ let us suppose that there is stationarity.
We success to reject the null hypothesis that states a unit root (non-stationarity) in the time serie.

This let us suppose that we could need to differenciate `paris_ts19`.

#### ACF

ACF for Autocorrelation Function

This graph is useful to identify the number of MA(q) (Moving Average) terms in our model.

We will interpret it as the following :

-   If the ACF shows a slow exponential or sinusoidal decay, it could suggests an AR process.
-   If the ACF cuts off after a specific delay (lag), it could suggests an AM process.

```{r, echo=FALSE}
# Plot of the Autocorrelation Function (ACF)
forecast::ggAcf(paris_ts19) +
  ggplot2::ggtitle("Sample ACF for CDG airport")
```

The ACF shows a gradual decline but with several significant spikes at various lags.
There are significant lags at intervals that could suggest a seasonal pattern, which is in line with our year seasonality.
Our data frequency is monthly, and these intervals correspond to the number 12.

The fact that there are significant autocorrelations at multiple lags might also suggest that our data is not stationary, and differencing (either seasonal or non-seasonal) might be required to achieve stationarity.

#### PACF

PACF for Partial Autocorrelation Function

This graph is useful to identify the number of AR(p) (AutoRegressive) terms in our model.

We will interpret it as the following :

-   If the PACF shows a slow exponential or sinusoidal decay, it could indicates an MA process.
-   If the PACF cuts off after a certain delay, it could indicates an AR process.

```{r, echo=FALSE}
# Plot of the Partial Autocorrelation Function (PACF)
forecast::ggPacf(paris_ts19) +
  ggplot2::ggtitle("Sample PACF for CDG airport")
```

The PACF shows a significant spike at lag 1 and then cuts off, which indicates that a non-seasonal AR(1) component may be present in our time series.
The other lags do not appear to be significantly different from zero, suggesting that no higher-order AR terms are needed.
:::

### Seasonal differencing

We will take the seasonality into account, which means that we make a difference at lag 12 because of our monthly data.

```{r}
# Difference
paris_ts19_diff <- diff(paris_ts19, lag = 12)

# Trend
ts_stl <- stl(paris_ts19_diff, s.window = "periodic")

# Plot
plot(paris_ts19_diff, col = 'blue', xlab = "Year", ylab = "Passengers",
     main = "CDG passengers after seasonal differencing")

lines(ts_stl$time.series[, "trend"], col = 'red')

legend("topright", legend = c("Original", "Trend"), col = c("blue", "red"), 
       lty = c(1, 1), cex = 0.8)
```

After differencing, we check again for stationarity.
Here, taking a look at the graph, it seems that there is no remaining trend.
To validate our proposal, we run the ADF test with the ACF and PACF graphs and look for stationarity.

::: panel-tabset
#### ADF

$H_0 : Unit root$ $H_1 : Stationary$

```{r}
adf_test2 <- adf.test(paris_ts19_diff, alternative = "stationary")

# Results of the ADF test
print(adf_test2)
```

Again, we reject the null hypothesis at a $5\%$ significance level.
Which means that we still have stationarity in our time series.

#### ACF

```{r, echo=FALSE}
# Plot of the Autocorrelation Function (ACF)
forecast::ggAcf(paris_ts19_diff) +
  ggplot2::ggtitle("Sample ACF for CDG airport after differencing")
```

#### PACF

```{r, echo=FALSE}
# Plot of the Partial Autocorrelation Function (PACF)
forecast::ggPacf(paris_ts19_diff) +
  ggplot2::ggtitle("Sample PACF for CDG airport after differencing")
```
:::

# Modeling

## PARIS

### Automated parameters choice


#### Model

R has an automated function that can help us to build our model.
We thus run it to get the suggested coefficients with : `auto.arima`.

```{r}
# Get suggestion
paris_ts19_auto_params <- forecast::auto.arima(paris_ts19)
```

Which gives us the following parameters : `r paris_ts19_auto_params`


### Manual parameters choice

::: panel-tabset
#### AIC

```{r, eval=FALSE}
# AIC parameters
paris_ts19_aic_params <- find_aic_params(paris_ts19)
# do not forget to save this new variable when you run this command
```


The Aikaike Information Criterion : the goal is to minimize this criterion with the different parameters of our model.
The parameters obtained are the following : `r paris_ts19_aic_params` *(p,d,q)x(P,D,Q)[12]*


#### BIC

```{r, eval=FALSE}}
# BIC parameters
paris_ts19_bic_params <- find_bic_params(paris_ts19)
# do not forget to save this new variable when you run this command
```

The Bayesian Information Criterion : the goal is to minimize this criterion with the different parameters of our model.
The parameters obtained are the following : `r paris_ts19_bic_params` *(p,d,q)x(P,D,Q)[12]*

:::

## MADRID

::: panel-tabset
#### AIC

```{r, eval=FALSE}
# AIC parameters
madrid_ts19_aic_params <- find_aic_params(madrid_ts19)
# do not forget to save this new variable when you run this command
```


The Aikaike Information Criterion : the goal is to minimize this criterion with the different parameters of our model.
The parameters obtained are the following : `r madrid_ts19_aic_params` *(p,d,q)x(P,D,Q)[12]*


#### BIC

```{r, eval=FALSE}}
# BIC parameters
madrid_ts19_bic_params <- find_bic_params(madrid_ts19)
# do not forget to save this new variable when you run this command
```

The Bayesian Information Criterion : the goal is to minimize this criterion with the different parameters of our model.
The parameters obtained are the following : `r madrid_ts19_bic_params` *(p,d,q)x(P,D,Q)[12]*

:::

## ROMA

::: panel-tabset
#### AIC

```{r, eval=FALSE}
# AIC parameters
roma_ts19_aic_params <- find_aic_params(roma_ts19)
# do not forget to save this new variable when you run this command
```


The Aikaike Information Criterion : the goal is to minimize this criterion with the different parameters of our model.
The parameters obtained are the following : `r roma_ts19_aic_params` *(p,d,q)x(P,D,Q)[12]*


#### BIC

```{r, eval=FALSE}}
# BIC parameters
roma_ts19_bic_params <- find_bic_params(roma_ts19)
# do not forget to save this new variable when you run this command
```

The Bayesian Information Criterion : the goal is to minimize this criterion with the different parameters of our model.
The parameters obtained are the following : `r roma_ts19_bic_params` *(p,d,q)x(P,D,Q)[12]*

:::

## COPENHAGEN

::: panel-tabset
#### AIC

```{r, eval=FALSE}
# AIC parameters
copenhagen_ts19_aic_params <- find_aic_params(copenhagen_ts19)
# do not forget to save this new variable when you run this command
```


The Aikaike Information Criterion : the goal is to minimize this criterion with the different parameters of our model.
The parameters obtained are the following : `r copenhagen_ts19_aic_params` *(p,d,q)x(P,D,Q)[12]*


#### BIC

```{r, eval=FALSE}
# BIC parameters
copenhagen_ts19_bic_params <- find_bic_params(copenhagen_ts19)
# do not forget to save this new variable when you run this command
```

The Bayesian Information Criterion : the goal is to minimize this criterion with the different parameters of our model.
The parameters obtained are the following : `r copenhagen_ts19_bic_params` *(p,d,q)x(P,D,Q)[12]*

:::

## OSLO

::: panel-tabset
#### AIC

```{r, eval=FALSE}
# AIC parameters
oslo_ts19_aic_params <- find_aic_params(oslo_ts19)
# do not forget to save this new variable when you run this command
```


The Aikaike Information Criterion : the goal is to minimize this criterion with the different parameters of our model.
The parameters obtained are the following : `r oslo_ts19_aic_params` *(p,d,q)x(P,D,Q)[12]*


#### BIC

```{r, eval=FALSE}
# BIC parameters
oslo_ts19_bic_params <- find_bic_params(oslo_ts19)
# do not forget to save this new variable when you run this command
```

The Bayesian Information Criterion : the goal is to minimize this criterion with the different parameters of our model.
The parameters obtained are the following : `r oslo_ts19_bic_params` *(p,d,q)x(P,D,Q)[12]*

:::


```{r, eval=FALSE}
# Here we save all our parameters
save(paris_ts19_aic_params, paris_ts19_bic_params,
     madrid_ts19_aic_params, madrid_ts19_bic_params,
     roma_ts19_aic_params, roma_ts19_bic_params,
     copenhagen_ts19_aic_params, copenhagen_ts19_bic_params,
     oslo_ts19_aic_params, oslo_ts19_bic_params,
     file = "hyperparameters.RData")
```


# SARIMA

## Model for PARIS

#### Parameters with AIC

::: panel-tabset
#### Model

Parameters used : 'r paris_ts19_aic_params$min_AIC_params` *(p,d,q)x(P,D,Q)[12]*

```{r}
# Setting the parameters
non_seasonal_order <- paris_ts19_aic_params$min_AIC_params[1:3]
seasonal_order <- paris_ts19_aic_params$min_AIC_params[4:6]
# Fitting the model
sarima_model_aic_paris <- Arima(paris_ts19, order=non_seasonal_order, seasonal=list(order=seasonal_order, period=12))
```

#### Residuals

Interpretation of the residuals :

```{r}
sarima_model_aic_paris_residuals <- checkresiduals(sarima_model_aic_paris)
p_value <- sarima_model_aic_paris_residuals$p.value
checkresiduals(sarima_model_aic_paris)
```

-   **The Ljung-Box test**, if we get $p-value < 0.05$, we reject the null hypothesis.
    This suggests that the error has a significant autocorrelation at some of the lags used in the test (up to lag 24 here).
    Then our model would not capture adequately the time-dependency.
    here, we have $p-value = `r p_value`$.

-   **The residuals time series plot**, helps us to check if our residuals look like white noise (meaning that our model has captured the underlying process correctly).
    Here, our residuals fluctuate around zero, we do not see obvious patterns which is a good sign.

-   **The ACF of residuals**, helps us to check the autocorrelation within our residuals.
    In a good model we would expect the bars to be between both dashed blue lines.
    In our case, we see some of the bars outside the confidence intervals.
    Thus, there may be some autocorrelation in the residuals that our model does not capture.

-   **The histogram and density plot**, shows the distribution of the residuals along with the density curve of the normal distribution for comparison.
    By assumption, our residuals should look like a normal distribution with a mean of zero.
    Here, we have large pikes around zero, but also some big deviations around.
    Our residuals may not be normally distributed.

#### Prediction

```{r}
# Prediction of the 12 next months
forecasts_aic_paris <- forecast(sarima_model_aic_paris, h=41)
#str(forecasts)
plot(forecasts_aic_paris)
```

#### Summary

```{r}
# Information on the model
sarima_model_aic_paris
```
#### Difference

We first plot the forecasted data and the actual data to see the difference.

```{r, echo=FALSE} 
# Forecast the next 32 periods
forecasted_values_paris <- forecast(sarima_model_aic_paris, h=41)

# For actual data: Create a tibble/data frame with time and actual values
actual_data_paris <- tibble(
  time = as.Date(time(paris_ts)),
  Value = as.vector(paris_ts),
  Type = 'Actual'
)

actual_data_paris <- actual_data_paris[actual_data_paris$time <= as.Date("2023-05-01"), ]

# For forecasted data: Create a tibble/data frame with forecasted times and values
forecast_data_paris <- tibble(
  time = as.Date(time(forecasted_values_paris$mean)),
  Value = as.vector(forecasted_values_paris$mean),
  Type = 'Forecast SARIMA'
)

# Combine actual and forecasted data
combined_data_paris <- bind_rows(actual_data_paris, forecast_data_paris)

# Plot using ggplot2
ggplot(data = combined_data_paris, aes(x = time, y = Value, color = Type)) +
  geom_line() +
  labs(title = "Charles de Gaulles passenger traffic forecast", x = "Time", y = "Value") +
  theme_minimal()

```

Then we plot the difference between the forecasted and actual values.

```{r, echo=FALSE}
# Our actual data on the orecast period
actual_data_subset_paris <- actual_data_paris[actual_data_paris$time >= as.Date("2020-01-01"),]

# Calculate the difference between forecasted and actual values
forecast_data_paris$diff <- forecast_data_paris$Value - actual_data_subset_paris$Value

# Plot the difference using ggplot2
ggplot(data = forecast_data_paris, aes(x = time, y = diff, color = Type)) +
  geom_line() +
  labs(title = "Difference between Forecasted and Actual Values", x = "Time", y = "Difference") +
  theme_minimal()
```

The we divide this difference by the forecasted value to see the scaled difference.


```{r, echo=FALSE}
# Scaling the difference
forecast_data_paris$scalediff <- forecast_data_paris$diff / forecast_data_paris$Value

# Plot the scaled difference using ggplot2
ggplot(data = forecast_data_paris, aes(x = time, y = scalediff, color = Type)) +
  geom_line() +
  labs(title = "Scaled difference between Forecasted and Actual Values", x = "Time", y = "Difference") +
  theme_minimal()
```

:::


#### Parameters with BIC

::: panel-tabset
#### Model

```{r}
print(min_BIC_params_paris)
```

```{r}
sarima_model_bic_paris <- Arima(paris_ts19, order=c(0,1,1), seasonal=list(order=c(0,1,1), period=12))
```

#### Residuals

Interpretation of the residuals :

```{r}
checkresiduals(sarima_model_bic_paris)
```

#### Prediction

```{r}
# Prediction of the 12 next months
forecasts_bic_paris <- forecast(sarima_model_bic_paris, h=41)
#str(forecasts)
plot(forecasts_bic_paris)
```

#### Summary

```{r}
# Information on the model
sarima_model_bic_paris
```
:::

## Model for MADRID

#### Parameters with AIC

::: panel-tabset
#### Model

```{r}
print(min_AIC_params_madrid)
```

```{r}
sarima_model_aic_madrid <- Arima(madrid_ts19, order=c(2,1,2), seasonal=list(order=c(0,1,1), period=12))
```

#### Residuals

Interpretation of the residuals :

```{r}
checkresiduals(sarima_model_aic_madrid)
```

#### Prediction

```{r}
# Prediction of the 12 next months
forecasts_aic_madrid <- forecast(sarima_model_aic_madrid, h=41)
#str(forecasts)
plot(forecasts_aic_madrid)
```

#### Summary

```{r}
# Information on the model
sarima_model_aic_madrid
```

```{r}

#Forecast the next 41 periods

forecasted_values_madrid <- forecast(sarima_model_aic_madrid, h=41)

#For actual data: Create a tibble/data frame with time and actual values

actual_data_madrid <- tibble(
time = as.Date(time(madrid_ts)),
Value = as.vector(madrid_ts),
Type = 'Actual'
)

actual_data_madrid <- actual_data_madrid[actual_data_madrid$time <= as.Date("2023-05-01"), ]

#For forecasted data: Create a tibble/data frame with forecasted times and values

forecast_data_madrid <- tibble(
time = as.Date(time(forecasted_values_madrid$mean)),
Value = as.vector(forecasted_values_madrid$mean),
Type = 'Forecast SARIMA'
)

#Combine actual and forecasted data

combined_data_madrid <- bind_rows(actual_data_madrid, forecast_data_madrid)

#Plot using ggplot2

ggplot(data = combined_data_madrid, aes(x = time, y = Value, color = Type)) +
geom_line() +
labs(title = "Madrid passenger traffic forecast", x = "Time", y = "Value") +
theme_minimal()
```

#### Difference

We take the same steps as for Paris CDG

```{r}

#Plot of the difference between prediction and actual data

actual_data_subset_madrid <- actual_data_madrid[actual_data_madrid$time >= as.Date("2020-01-01"),]

#Calculate the difference between forecasted and actual values

forecast_data_madrid$diff <- forecast_data_madrid$Value - actual_data_subset_madrid$Value

#Plot the difference using ggplot2

ggplot(data = forecast_data_madrid, aes(x = time, y = diff, color = Type)) +
geom_line() +
labs(title = "Difference between Forecasted and Actual Values", x = "Time", y = "Difference") +
theme_minimal()
```

```{r}

#Plot of the scaled difference between prediction and actual data

forecast_data_madrid$scalediff <- forecast_data_madrid$diff / forecast_data_madrid$Value

ggplot(data = forecast_data_madrid, aes(x = time, y = scalediff, color = Type)) +
geom_line() +
labs(title = "Scaled difference between Forecasted and Actual Values", x = "Time", y = "Difference") +
theme_minimal()
```

:::

#### Parameters with BIC

::: panel-tabset
#### Model

```{r}
print(min_BIC_params_madrid)
```

```{r}
sarima_model_bic_madrid <- Arima(madrid_ts19, order=c(0,1,1), seasonal=list(order=c(0,1,1), period=12))
```

#### Residuals

Interpretation of the residuals :

```{r}
checkresiduals(sarima_model_bic_madrid)
```

#### Prediction

```{r}
# Prediction of the 12 next months
forecasts_bic_madrid <- forecast(sarima_model_bic_madrid, h=41)
#str(forecasts)
plot(forecasts_bic_madrid)
```

#### Summary

```{r}
# Information on the model
sarima_model_bic_madrid
```
:::

## Model for ROMA

#### Parameters with AIC

::: panel-tabset
#### Model

```{r}
print(min_AIC_params_roma)
```

```{r}
sarima_model_aic_roma <- Arima(roma_ts19, order=c(1,1,2), seasonal=list(order=c(0,1,1), period=12))
```

#### Residuals

Interpretation of the residuals :

```{r}
checkresiduals(sarima_model_aic_roma)
```

#### Prediction

```{r}
# Prediction of the 12 next months
forecasts_aic_roma <- forecast(sarima_model_aic_roma, h=41)
#str(forecasts)
plot(forecasts_aic_roma)
```

#### Summary

```{r}
# Information on the model
sarima_model_aic_roma
```
#### Difference

We take the same steps as for Paris CDG

```{r}
# Forecast the next 41 periods

forecasted_values_roma <- forecast(sarima_model_aic_roma, h=41)

# For actual data: Create a tibble/data frame with time and actual values

actual_data_roma <- tibble(
  time = as.Date(time(roma_ts)),
  Value = as.vector(roma_ts),
  Type = 'Actual'
)

actual_data_roma <- actual_data_roma[actual_data_roma$time <= as.Date("2023-05-01"), ]

# For forecasted data: Create a tibble/data frame with forecasted times and values

forecast_data_roma <- tibble(
  time = as.Date(time(forecasted_values_roma$mean)),
  Value = as.vector(forecasted_values_roma$mean),
  Type = 'Forecast SARIMA'
)

# Combine actual and forecasted data

combined_data_roma <- bind_rows(actual_data_roma, forecast_data_roma)

# Plot using ggplot2

ggplot(data = combined_data_roma, aes(x = time, y = Value, color = Type)) +
  geom_line() +
  labs(title = "Roma passenger traffic forecast", x = "Time", y = "Value") +
  theme_minimal()
```

```{r}


# Plot of the difference between prediction and actual data

actual_data_subset_roma <- actual_data_roma[actual_data_roma$time >= as.Date("2020-01-01"),]

# Calculate the difference between forecasted and actual values

forecast_data_roma$diff <- forecast_data_roma$Value - actual_data_subset_roma$Value

# Plot the difference using ggplot2

ggplot(data = forecast_data_roma, aes(x = time, y = diff, color = Type)) +
  geom_line() +
  labs(title = "Difference between Forecasted and Actual Values", x = "Time", y = "Difference") +
  theme_minimal()
```

```{r}


# Plot of the scaled difference between prediction and actual data

forecast_data_roma$scalediff <- forecast_data_roma$diff / forecast_data_roma$Value

ggplot(data = forecast_data_roma, aes(x = time, y = scalediff, color = Type)) +
  geom_line() +
  labs(title = "Scaled difference between Forecasted and Actual Values", x = "Time", y = "Difference") +
  theme_minimal()
```
:::

#### Parameters with BIC

::: panel-tabset
#### Model

```{r}
print(min_BIC_params_roma)
```

```{r}
sarima_model_bic_roma <- Arima(roma_ts19, order=c(0,1,1), seasonal=list(order=c(0,1,1), period=12))
```

#### Residuals

Interpretation of the residuals :

```{r}
checkresiduals(sarima_model_bic_roma)
```

#### Prediction

```{r}
# Prediction of the 12 next months
forecasts_bic_roma <- forecast(sarima_model_bic_roma, h=41)
#str(forecasts)
plot(forecasts_bic_roma)
```

#### Summary

```{r}
# Information on the model
sarima_model_bic_roma
```
:::

## Model for COPENHAGEN

#### Parameters with AIC

::: panel-tabset
#### Model

```{r}
print(min_AIC_params_kob)
```

```{r}
sarima_model_aic_kob <- Arima(copenhagen_ts19, order=c(1,1,0), seasonal=list(order=c(0,1,2), period=12))
```

#### Residuals

Interpretation of the residuals :

```{r}
checkresiduals(sarima_model_aic_kob)
```

#### Prediction

```{r}
# Prediction of the 12 next months
sarima_model_aic_kob <- forecast(sarima_model_aic_kob, h=41)
#str(forecasts)
plot(sarima_model_aic_kob)
```

#### Summary

```{r}
# Information on the model
sarima_model_aic_kob
```
#### Difference

We take the same steps as for Paris CDG

```{r}


# Forecast the next 41 periods

forecasted_values_kob <- forecast(sarima_model_aic_kob, h=41)

# For actual data: Create a tibble/data frame with time and actual values

actual_data_kob <- tibble(
  time = as.Date(time(copenhagen_ts)),
  Value = as.vector(copenhagen_ts),
  Type = 'Actual'
)

actual_data_kob <- actual_data_kob[actual_data_kob$time <= as.Date("2023-05-01"), ]

# For forecasted data: Create a tibble/data frame with forecasted times and values

forecast_data_kob <- tibble(
  time = as.Date(time(forecasted_values_kob$mean)),
  Value = as.vector(forecasted_values_kob$mean),
  Type = 'Forecast SARIMA'
)

# Combine actual and forecasted data

combined_data_kob <- bind_rows(actual_data_kob, forecast_data_kob)

# Plot using ggplot2

ggplot(data = combined_data_kob, aes(x = time, y = Value, color = Type)) +
  geom_line() +
  labs(title = "Kob passenger traffic forecast", x = "Time", y = "Value") +
  theme_minimal()
```

```{r}

# Plot of the difference between prediction and actual data

actual_data_subset_kob <- actual_data_kob[actual_data_kob$time >= as.Date("2020-01-01"),]

# Calculate the difference between forecasted and actual values

forecast_data_kob$diff <- forecast_data_kob$Value - actual_data_subset_kob$Value

# Plot the difference using ggplot2

ggplot(data = forecast_data_kob, aes(x = time, y = diff, color = Type)) +
  geom_line() +
  labs(title = "Difference between Forecasted and Actual Values", x = "Time", y = "Difference") +
  theme_minimal()

```

```{r}


# Plot of the scaled difference between prediction and actual data

forecast_data_kob$scalediff <- forecast_data_kob$diff / forecast_data_kob$Value

ggplot(data = forecast_data_kob, aes(x = time, y = scalediff, color = Type)) +
  geom_line() +
  labs(title = "Scaled difference between Forecasted and Actual Values", x = "Time", y = "Difference") +
  theme_minimal()
```
:::

#### Parameters with BIC

::: panel-tabset
#### Model

```{r}
print(min_BIC_params_kob)
```

```{r}
sarima_model_bic_kob <- Arima(copenhagen_ts19, order=c(0,1,0), seasonal=list(order=c(0,1,1), period=12))
```

#### Residuals

Interpretation of the residuals :

```{r}
checkresiduals(sarima_model_bic_kob)
```

#### Prediction

```{r}
# Prediction of the 12 next months
forecasts_bic_kob <- forecast(sarima_model_bic_kob, h=41)
#str(forecasts)
plot(forecasts_bic_kob)
```

#### Summary

```{r}
# Information on the model
sarima_model_bic_kob
```
:::

## Model for OSLO

#### Parameters with AIC

::: panel-tabset
#### Model

```{r}
print(min_AIC_params_oslo)
```

```{r}
sarima_model_aic_oslo <- Arima(oslo_ts19, order=c(0,1,1), seasonal=list(order=c(0,1,1), period=12))
```

#### Residuals

Interpretation of the residuals :

```{r}
checkresiduals(sarima_model_aic_oslo)
```

#### Prediction

```{r}
# Prediction of the 12 next months
forecasts_aic_oslo <- forecast(sarima_model_aic_oslo, h=41)
#str(forecasts)
plot(forecasts_aic_oslo)
```

#### Summary

```{r}
# Information on the model
sarima_model_aic_oslo
```
#### Difference

We take the same steps as for Paris CDG

```{r}


# Forecast the next 41 periods

forecasted_values_oslo <- forecast(sarima_model_aic_oslo, h=41)

# For actual data: Create a tibble/data frame with time and actual values

actual_data_oslo <- tibble(
  time = as.Date(time(oslo_ts)),
  Value = as.vector(oslo_ts),
  Type = 'Actual'
)

actual_data_oslo <- actual_data_oslo[actual_data_oslo$time <= as.Date("2023-05-01"), ]

# For forecasted data: Create a tibble/data frame with forecasted times and values

forecast_data_oslo <- tibble(
  time = as.Date(time(forecasted_values_oslo$mean)),
  Value = as.vector(forecasted_values_oslo$mean),
  Type = 'Forecast SARIMA'
)

# Combine actual and forecasted data

combined_data_oslo <- bind_rows(actual_data_oslo, forecast_data_oslo)

# Plot using ggplot2

ggplot(data = combined_data_oslo, aes(x = time, y = Value, color = Type)) +
  geom_line() +
  labs(title = "Oslo passenger traffic forecast", x = "Time", y = "Value") +
  theme_minimal()
```

```{r}


# Plot of the difference between prediction and actual data

actual_data_subset_oslo <- actual_data_oslo[actual_data_oslo$time >= as.Date("2020-01-01"),]

# Calculate the difference between forecasted and actual values

forecast_data_oslo$diff <- forecast_data_oslo$Value - actual_data_subset_oslo$Value

# Plot the difference using ggplot2

ggplot(data = forecast_data_oslo, aes(x = time, y = diff, color = Type)) +
  geom_line() +
  labs(title = "Difference between Forecasted and Actual Values", x = "Time", y = "Difference") +
  theme_minimal()
```

```{r}


# Plot of the scaled difference between prediction and actual data

forecast_data_oslo$scalediff <- forecast_data_oslo$diff / forecast_data_oslo$Value

ggplot(data = forecast_data_oslo, aes(x = time, y = scalediff, color = Type)) +
  geom_line() +
  labs(title = "Scaled difference between Forecasted and Actual Values", x = "Time", y = "Difference") +
  theme_minimal()
```

:::

#### Parameters with BIC

::: panel-tabset

#### Model

```{r}
print(min_BIC_params_oslo)
```

```{r}
sarima_model_bic_oslo <- Arima(oslo_ts19, order=c(0,1,1), seasonal=list(order=c(0,1,1), period=12))
```

#### Residuals

Interpretation of the residuals :

```{r}
checkresiduals(sarima_model_bic_oslo)
```

#### Prediction

```{r}
# Prediction of the 12 next months
forecasts_bic_oslo <- forecast(sarima_model_bic_oslo, h=41)
#str(forecasts)
plot(forecasts_bic_oslo)
```

#### Summary

```{r}
# Information on the model
sarima_model_bic_oslo
```
:::

# Comparing the difference

The scale differences are displayed on the same plot for comparison.

```{r}
# Combine all forecast data
combined_data_all <- bind_rows(
  mutate(forecast_data_paris, City = "Paris"),
  mutate(forecast_data_madrid, City = "Madrid"),
  mutate(forecast_data_roma, City = "Roma"),
  mutate(forecast_data_kob, City = "Kob"),
  mutate(forecast_data_oslo, City = "Oslo")
)

# Plot using ggplot2
ggplot(data = combined_data_all, aes(x = time, y = scalediff, color = City)) +
  geom_line() +
  labs(title = "Scaled difference between Forecasted and Actual Values",
       x = "Time", y = "Difference") +
  theme_minimal()
```



### Politics

We add 3 politics to the dataset : - Borders main UE Period - Border non-UE Period - Negative tests Period

We add them as dummies over the same period of our time series.
We put a 1 when the policy is applied, and 0 when it is not.
We did it on Excel, it was simplier than with R.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
# Global dataset
airports_politics <- openxlsx::read.xlsx(xlsxFile="datasets/DATA_POLITICS.xlsx")

# Dataset for politics and airports
col_start <- which(names(airports_politics) == "2002-01")
col_end <- which(names(airports_politics) == "2023-09")
library(dplyr)
airports <- airports_politics %>% dplyr::select(1, col_start : col_end)
politics <- airports_politics %>% dplyr::select(1, (col_end+1):length(airports_politics))

# Dataset for each policy

#a
col_start_a <- which(names(airports_politics) == "2002-01.a")
col_end_a <- which(names(airports_politics) == "2023-09.a")
policy_a <- airports_politics %>% dplyr::select(1, col_start_a : col_end_a)

#b
col_start_b <- which(names(airports_politics) == "2002-01.b")
col_end_b <- which(names(airports_politics) == "2023-09.b")
policy_b <- airports_politics %>% dplyr::select(1, col_start_b : col_end_b)

#c
col_start_c <- which(names(airports_politics) == "2002-01.c")
col_end_c <- which(names(airports_politics) == "2023-09.c")
policy_c <- airports_politics %>% dplyr::select(1, col_start_c : col_end_c)
```

```{r, warning=FALSE, message=FALSE, echo=FALSE}
# Pivot
policy_a <- tidyr::pivot_longer(policy_a, cols = -c("Airport"), names_to = "Date", values_to = "Borders main EU period")
policy_b <- tidyr::pivot_longer(policy_b, cols = -c("Airport"), names_to = "Date", values_to = "Borders non-EU period")
policy_c <- tidyr::pivot_longer(policy_c, cols = -c("Airport"), names_to = "Date", values_to = "Negative tests period")
airports <- tidyr::pivot_longer(airports, cols = -c("Airport"), names_to = "Date", values_to = "Passenger")

# Formating date
policy_a$Date <- gsub("\\.a$", "", policy_a$Date)
policy_b$Date <- gsub("\\.b$", "", policy_b$Date)
policy_c$Date <- gsub("\\.c$", "", policy_c$Date)
```

```{r, echo=FALSE}
# Merging
politics_formate <- merge(airports, policy_a, by = c("Airport", "Date"), all.x = TRUE)
politics_formate <- merge(politics_formate, policy_b, by = c("Airport", "Date"), all.x = TRUE)
politics_formate <- merge(politics_formate, policy_c, by = c("Airport", "Date"), all.x = TRUE)

# Date format
politics_formate$Date <- zoo::as.Date(paste0(politics_formate$Date, "-01"), format="%Y-%m-%d")
```

```{r, echo=FALSE}
# Kopenhagen
kopen <- politics_formate %>% dplyr::filter(Airport == "KOBENHAVN/KASTRUP airport")
# Madrid
madrid <- politics_formate %>% dplyr::filter(Airport == "ADOLFO SUAREZ MADRID-BARAJAS airport")
# Oslo
oslo <- politics_formate %>% dplyr::filter(Airport == "OSLO/GARDERMOEN airport")
# Paris
paris <- politics_formate %>% dplyr::filter(Airport == "PARIS-CHARLES DE GAULLE airport")
# Roma
roma <- politics_formate %>% dplyr::filter(Airport == "ROMA/FIUMICINO airport")
```

::: panel-tabset
#### Kopenhaven

```{r, echo=FALSE}
styled_dt(kopen)
```

#### Madrid

```{r, echo=FALSE}
styled_dt(madrid)
```

#### Oslo

```{r, echo=FALSE}
styled_dt(oslo)
```

#### Paris

```{r, echo=FALSE}
styled_dt(paris)
```

#### Roma

```{r, echo=FALSE}
styled_dt(roma)
```
:::

### New SARIMA model for Paris

#### Model

```{r}
# Ensure the external regressors are in the correct format
#paris_20 <- paris[paris$Date<="2020-09-01"]
#external_regressors <- as.matrix(paris[, c('Borders main EU period', 'Borders non-EU period', 'Negative tests period')])

# Fit the SARIMAX model
#auto.arima(paris_ts20, xreg = external_regressors, seasonal = TRUE)
```

```{r}
sarima_model_aic_paris2 <- Arima(paris_ts20, order=c(0,1,1), seasonal=list(order=c(0,1,1), period=12))
```

#### Residuals

Interpretation of the residuals :

```{r}
checkresiduals(sarima_model_aic_paris2)
```

#### Prediction

```{r}
# Forecast the next 32 periods
forecasted_values <- forecast(sarima_model_aic_paris, h=32)

# For actual data: Create a tibble/data frame with time and actual values
actual_data <- tibble(
  time = as.Date(time(paris_ts)),
  Value = as.vector(paris_ts),
  Type = 'Actual'
)

# For forecasted data: Create a tibble/data frame with forecasted times and values
forecast_data <- tibble(
  time = as.Date(time(forecasted_values$mean)),
  Value = as.vector(forecasted_values$mean),
  Type = 'Forecast'
)

# Combine actual and forecasted data
combined_data <- bind_rows(actual_data, forecast_data)

# Plot using ggplot2
ggplot(data = combined_data, aes(x = time, y = Value, color = Type)) +
  geom_line() +
  labs(title = "Charles de Gaulles passenger traffic forecast", x = "Time", y = "Value") +
  theme_minimal()

```

#### Summary

```{r}
# Information on the model
sarima_model_aic_paris2
```









